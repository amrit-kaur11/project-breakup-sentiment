# -*- coding: utf-8 -*-
"""Model-Centric — DistilBERT Fine-Tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EMV7nE50CWeXI3lPeUo82r7JFAIspRi-

# **Milestone-2: Model-Centric — DistilBERT Fine-Tuning**

**Domain-Specific Sentiment Analysis with Low-Resource NLP Techniques — Model-Centric Notebook**

This notebook implements the Model-Centric approach: fine-tuning a pretrained transformer (DistilBERT) on your Milestone-1 labeled Reddit break-up dataset. It is organized, reproducible, and Colab-ready: tokenization, class weighting for imbalance, training with early stopping, evaluation (validation + test), artifact saving (best model + tokenizer + training logs), and a simple inference utility for later UI integration.

**How to use:**
1. Upload `reddit_breakup_dataset_labeled.xlsx` to the notebook working directory, or can change it to csv file.
2. Run the cells in order. The notebook creates an `output_dir/` folder and saves artifacts there.

---
"""

# ===== INSTALLATIONS =====
!pip -q install -U transformers datasets accelerate scikit-learn torch pandas

# ===== IMPORTS =====
import random, numpy as np, pandas as pd, torch, matplotlib.pyplot as plt, warnings
import os, json, random, itertools
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_fscore_support
from sklearn.utils.class_weight import compute_class_weight
import torch
import torch.utils.data as tud   # << use this alias instead of 'from torch.utils.data import Dataset'
from datasets import Dataset, DatasetDict
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          TrainingArguments, Trainer, EarlyStoppingCallback, set_seed)

warnings.filterwarnings('ignore')

# ===== SEED & DEVICE =====
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)
device = "cuda" if torch.cuda.is_available() else "cpu"
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

# -------------------- Config (you can edit) --------------------
SEED = 42
BASE_MODEL = "distilbert-base-uncased"
WARMUP_RATIO = 0.1
WEIGHT_DECAY = 0.01
LABEL_SMOOTHING = 0.05
STRIDE = 64               # token overlap between chunks
OUTPUT_DIR = "distilbert_out_chunked"

# Search space (small & safe)
LR_GRID = [2e-5]
FREEZE_GRID = [2]
BATCH_GRID = [8]
MAXLEN_GRID = [256]

# collapse the original labels into 4 broad categories

LABEL_MAP = {
    "anger": "anger", "rage": "anger", "resentment": "anger",
    "sadness": "sadness", "grief": "sadness", "depression": "sadness",
    "relief": "relief", "acceptance": "relief",
    "confusion": "confusion", "mixed": "confusion",
}
CLASSES = ["anger", "confusion", "relief", "sadness"]
LABEL2ID = {c:i for i,c in enumerate(CLASSES)}
ID2LABEL = {i:c for c,i in LABEL2ID.items()}

set_seed(SEED); random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

# ===== LOAD DATASET =====
def load_df(path: str) -> pd.DataFrame:
    ext = os.path.splitext(path)[1].lower()
    df = pd.read_excel(path) if ext in (".xlsx", ".xls") else pd.read_csv(path)
    df.columns = [c.strip() for c in df.columns]
    lowers = {c.lower(): c for c in df.columns}
    if "text" not in df.columns:
        if "text" in lowers: df = df.rename(columns={lowers["text"]:"text"})
        elif "body" in lowers: df = df.rename(columns={lowers["body"]:"text"})
        else: raise KeyError("Need a 'text' or 'body' column.")
    if "label" not in df.columns:
        if "label" in lowers: df = df.rename(columns={lowers["label"]:"label"})
        else: raise KeyError("Need a 'label' column.")

    df["text"] = df["text"].astype(str).str.strip()
    df = df.dropna(subset=["text","label"])
    df = df[df["text"].str.len() > 0].reset_index(drop=True)
    df["label"] = df["label"].map(LABEL_MAP)
    df = df.dropna(subset=["label"]).reset_index(drop=True)
    return df

DATA_FILE = "/content/reddit_breakup_labeled.xlsx"  # change if needed
df_preview = load_df(DATA_FILE)
print("Rows:", len(df_preview))
print(df_preview["label"].value_counts())
df_preview.head(3)

import torch
from torch.utils.data import Dataset as TudDataset # Use an alias to avoid conflict with datasets.Dataset
from transformers import AutoTokenizer
from typing import Dict, List, Optional
from dataclasses import dataclass

class ChunkedDataset(TudDataset): # Inherit from torch.utils.data.Dataset
    def __init__(self, texts, labels, tokenizer, max_len, stride, keep_groups):
        self.input_ids = []
        self.attention_mask = []
        self.labels = []
        self.group_ids = [] # Keep track of original sample index

        for idx, (t, y) in enumerate(zip(texts, labels)):
            # Handle potential None values in texts
            if not isinstance(t, str):
                t = str(t)

            enc = tokenizer(
                t,
                truncation=True,
                max_length=max_len,
                return_overflowing_tokens=True,
                stride=stride,
                padding=False, # Don't pad here, will do in collator
            )
            for k in range(len(enc["input_ids"])):
                self.input_ids.append(enc["input_ids"][k]) # Append list of integers
                self.attention_mask.append(enc["attention_mask"][k]) # Append list of integers
                self.labels.append(y) # Append integer label
                if keep_groups:
                    self.group_ids.append(idx)

    def __len__(self): return len(self.input_ids)

    def __getitem__(self, i):
        return {
            "input_ids": self.input_ids[i],
            "attention_mask": self.attention_mask[i],
            "labels": self.labels[i],
        }

# Custom Data Collator for padding
@dataclass
class DataCollatorWithPadding:
    tokenizer: AutoTokenizer
    padding = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    return_tensors: str = "pt"

    def __call__(self, features: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:
        batch = self.tokenizer.pad(
            features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors=self.return_tensors,
        )
        # If labels are present, convert them to tensor
        if "labels" in batch:
            batch["labels"] = batch["labels"].long()
        return batch

# ===== CLASS WEIGHTS =====
def compute_class_weights(y: List[int], num_classes: int) -> torch.Tensor:
    counts = np.bincount(y, minlength=num_classes)
    w = 1.0 / (counts + 1e-6)
    w = w * (num_classes / w.sum())
    return torch.tensor(w, dtype=torch.float)

# ===== WEIGHT TRAINERS =====
class WeightedTrainer(Trainer):
    def __init__(self, *args, class_weights=None, label_smoothing: float = 0.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights
        self.label_smoothing = label_smoothing

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None
        loss_fct = torch.nn.CrossEntropyLoss(weight=weight, label_smoothing=self.label_smoothing)
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

# ===== Metrics (chunk-level; aggregate separately) =====
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {"accuracy": accuracy_score(labels, preds), "macro_f1": f1_score(labels, preds, average="macro")}

# ===== Aggregation helpers =====
def aggregate_logits_by_group(chunk_logits: np.ndarray, group_ids: List[int], num_groups: int) -> np.ndarray:
    sums = np.zeros((num_groups, chunk_logits.shape[1]), dtype=np.float64)
    counts = np.zeros(num_groups, dtype=np.int32)
    for logit, gid in zip(chunk_logits, group_ids):
        sums[gid] += logit; counts[gid] += 1
    counts[counts==0] = 1
    return sums / counts[:,None]

def eval_aggregated(trainer: Trainer, ds: ChunkedDataset, orig_labels: List[int]) -> Dict[str, float]:
    pred = trainer.predict(ds)
    logits = pred.predictions
    group_ids = ds.group_ids
    num_groups = len(orig_labels)
    agg_logits = aggregate_logits_by_group(logits, group_ids, num_groups)
    preds = agg_logits.argmax(axis=1)
    acc = accuracy_score(orig_labels, preds)
    macro_f1 = f1_score(orig_labels, preds, average="macro")
    return {"accuracy": acc, "macro_f1": macro_f1, "preds": preds, "logits": agg_logits}

# ===== BUILD AND TRAIN =====
def train_one_combo(df: pd.DataFrame, lr: float, freeze_layers: int, batch: int, max_len: int, out_dir: str):
    texts = df["text"].tolist()
    y = [LABEL2ID[l] for l in df["label"].tolist()]
    x_train, x_temp, y_train, y_temp = train_test_split(texts, y, test_size=0.2, stratify=y, random_state=SEED)
    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED)

    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=len(CLASSES), id2label=ID2LABEL, label2id=LABEL2ID)

    # Freeze lower layers
    for i, layer in enumerate(model.distilbert.transformer.layer):
        if i < freeze_layers:
            for p in layer.parameters(): p.requires_grad = False

    train_ds = ChunkedDataset(x_train, y_train, tok, max_len=max_len, stride=STRIDE, keep_groups=False)
    val_ds   = ChunkedDataset(x_val,   y_val,   tok, max_len=max_len, stride=STRIDE, keep_groups=True)
    test_ds  = ChunkedDataset(x_test,  y_test,  tok, max_len=max_len, stride=STRIDE, keep_groups=True)

    class_w = compute_class_weights(y_train, len(CLASSES))
    args = TrainingArguments(
        output_dir=out_dir, learning_rate=lr, num_train_epochs=8,
        per_device_train_batch_size=batch, per_device_eval_batch_size=batch,
        eval_strategy="epoch", save_strategy="epoch",
        load_best_model_at_end=True, metric_for_best_model="macro_f1", greater_is_better=True,
        weight_decay=WEIGHT_DECAY, warmup_ratio=WARMUP_RATIO, logging_steps=25,
        report_to="none", seed=SEED
    )

    # Instantiate the custom data collator
    data_collator = DataCollatorWithPadding(tokenizer=tok)

    trainer = WeightedTrainer(
        model=model, args=args,
        train_dataset=train_ds, eval_dataset=val_ds,
        compute_metrics=compute_metrics,
        class_weights=class_w, label_smoothing=LABEL_SMOOTHING,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
        data_collator=data_collator # Provide the data collator to the Trainer
    )
    trainer.train()

    # Aggregated metrics
    def agg_eval(ds, y_true):
        pred = trainer.predict(ds); logits = pred.predictions
        group_ids = ds.group_ids # Access group_ids from the ChunkedDataset object
        agg = aggregate_logits_by_group(logits, group_ids, len(y_true))
        preds = agg.argmax(axis=1)
        return {
            "accuracy": accuracy_score(y_true, preds),
            "macro_f1": f1_score(y_true, preds, average="macro"),
            "preds": preds, "y_true": y_true
        }

    val_agg  = agg_eval(val_ds,  y_val)
    test_agg = agg_eval(test_ds, y_test)
    return {"val": val_agg, "test": test_agg}, trainer, tok, (x_test, y_test, test_ds)

# ===== Grid search =====
def grid_search_and_train(data_file: str, out_dir: str = OUTPUT_DIR):
    os.makedirs(out_dir, exist_ok=True)
    # remove any old results to avoid stale schemas
    jpath = os.path.join(out_dir, "grid_results.json")
    if os.path.exists(jpath):
        os.remove(jpath)

    df = load_df(data_file)

    best = None
    best_combo = None
    rows = []

    combos = list(itertools.product(LR_GRID, FREEZE_GRID, BATCH_GRID, MAXLEN_GRID))
    print(f"Trying {len(combos)} combos...")
    for idx, (lr, freeze, batch, max_len) in enumerate(combos, 1):
        combo_dir = os.path.join(out_dir, f"gs_lr{lr}_fz{freeze}_bs{batch}_ml{max_len}")
        print(f"[{idx}/{len(combos)}] lr={lr}, freeze={freeze}, batch={batch}, max_len={max_len}")
        os.makedirs(combo_dir, exist_ok=True)

        try:
            res, trainer, tok, test_pack = train_one_combo(df, lr, freeze, batch, max_len, combo_dir)
        except RuntimeError as e:
            print("  Skipping (runtime error):", e)
            continue

        # FLAT logging (no nesting)
        rows.append({
            "lr": lr, "freeze": freeze, "batch": batch, "max_len": max_len,
            "val_acc":  res["val"]["accuracy"],
            "val_f1":   res["val"]["macro_f1"],
            "test_acc": res["test"]["accuracy"],
            "test_f1":  res["test"]["macro_f1"],
        })

        # model selection by VAL macro-F1
        if (best is None) or (res["val"]["macro_f1"] > best["val"]["macro_f1"]):
            best = res
            best_combo = {"lr": lr, "freeze": freeze, "batch": batch, "max_len": max_len}
            trainer.save_model(os.path.join(out_dir, "best"))
            tok.save_pretrained(os.path.join(out_dir, "best_tok"))

        print(f"  VAL: acc={res['val']['accuracy']:.3f} f1={res['val']['macro_f1']:.3f} | "
              f"TEST: acc={res['test']['accuracy']:.3f} f1={res['test']['macro_f1']:.3f}")

    # persist once at the end
    with open(os.path.join(out_dir, "grid_results.json"), "w") as f:
        json.dump(rows, f, indent=2)
    with open(os.path.join(out_dir, "best_summary.json"), "w") as f:
        # Convert numpy arrays to lists before saving
        if best is not None:
            for key in ["val", "test"]:
                if key in best:
                    if "preds" in best[key] and isinstance(best[key]["preds"], np.ndarray):
                        best[key]["preds"] = best[key]["preds"].tolist()
                    if "logits" in best[key] and isinstance(best[key]["logits"], np.ndarray):
                        best[key]["logits"] = best[key]["logits"].tolist()
                    if "y_true" in best[key] and isinstance(best[key]["y_true"], np.ndarray):
                         best[key]["y_true"] = best[key]["y_true"].tolist()

        json.dump({"best_combo": best_combo, "best": best}, f, indent=2)

    return best_combo, best

best = grid_search_and_train(DATA_FILE, out_dir=OUTPUT_DIR)

import os, json, pandas as pd

path = os.path.join(OUTPUT_DIR, "grid_results.json")
if not os.path.exists(path):
    raise FileNotFoundError(f"Missing {path}. Run the grid cell first.")

with open(path) as f:
    rows = json.load(f)

if not isinstance(rows, list) or len(rows) == 0:
    print("grid_results.json is empty — likely every combo was skipped. "
          "Try a smaller grid: LR_GRID=[2e-5]; FREEZE_GRID=[2]; BATCH_GRID=[8]; MAXLEN_GRID=[256]")
else:
    # normalize works for flat AND nested
    df = pd.json_normalize(rows, sep="_")
    # standardize column names if nested ones exist
    rename_map = {
        "val.macro_f1": "val_f1",
        "val_macro_f1": "val_f1",
        "val.accuracy": "val_acc",
        "val_accuracy": "val_acc",
        "test.macro_f1": "test_f1",
        "test_macro_f1": "test_f1",
        "test.accuracy": "test_acc",
        "test_accuracy": "test_acc",
    }
    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})

    # final sanity—if val_f1 still missing, show columns and stop
    if "val_f1" not in df.columns:
        print("Columns present:", list(df.columns))
        raise KeyError("No 'val_f1' column found. Check the printed columns above.")

    display(df.sort_values("val_f1", ascending=False).reset_index(drop=True))

with open(os.path.join(OUTPUT_DIR,"best_summary.json")) as f:
    print(json.dumps(json.load(f), indent=2))

import os, json, pandas as pd
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# reload data / split identically
df = load_df(DATA_FILE)
texts = df["text"].tolist()
y = [LABEL2ID[l] for l in df["label"].tolist()]
x_train, x_temp, y_train, y_temp = train_test_split(texts, y, test_size=0.2, stratify=y, random_state=SEED)
x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=SEED)

# load best combo
with open(os.path.join(OUTPUT_DIR,"best_summary.json")) as f:
    best_summary = json.load(f)["best_combo"]
max_len = best_summary["max_len"]; batch = best_summary["batch"]

tok = AutoTokenizer.from_pretrained(os.path.join(OUTPUT_DIR,"best_tok"))
mdl = AutoModelForSequenceClassification.from_pretrained(os.path.join(OUTPUT_DIR,"best"))

# Instantiate the custom data collator
data_collator = DataCollatorWithPadding(tokenizer=tok)

test_ds = ChunkedDataset(x_test, y_test, tok, max_len=max_len, stride=STRIDE, keep_groups=True)
args = TrainingArguments(output_dir="/content/tmp_eval", per_device_eval_batch_size=batch, report_to="none")


trainer = Trainer(model=mdl, args=args, data_collator=data_collator) # Pass the data collator here
pred = trainer.predict(test_ds)
agg = aggregate_logits_by_group(pred.predictions, test_ds.group_ids, len(y_test))
preds = agg.argmax(axis=1)

print("AGG TEST:",
      {"accuracy": accuracy_score(y_test, preds), "macro_f1": f1_score(y_test, preds, average='macro')})
print(classification_report(y_test, preds, target_names=CLASSES, digits=3))

cm = confusion_matrix(y_test, preds, labels=list(range(len(CLASSES))))
cm_norm = confusion_matrix(y_test, preds, labels=list(range(len(CLASSES))))

# === Plot & SAVE: counts ===
fig, ax = plt.subplots(figsize=(6,5))
im = ax.imshow(cm, cmap=plt.cm.Blues)
ax.figure.colorbar(im, ax=ax)
ax.set_xticks(np.arange(len(CLASSES))); ax.set_xticklabels(CLASSES, rotation=45, ha="right")
ax.set_yticks(np.arange(len(CLASSES))); ax.set_yticklabels(CLASSES)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(j, i, cm[i, j], ha="center", va="center", color="white" if cm[i, j] > cm.max()/2 else "black")
ax.set_xlabel("Predicted"); ax.set_ylabel("True"); ax.set_title("Confusion Matrix — Best Model")
plt.tight_layout(); plt.savefig(f"{OUTPUT_DIR}/confusion_matrix_counts.png", bbox_inches="tight"); plt.show()

# === Save metrics & report ===
from sklearn.metrics import classification_report, accuracy_score, f1_score
metrics = {
    "accuracy": float(accuracy_score(y_test, preds)),
    "macro_f1": float(f1_score(y_test, preds, average="macro"))
}
with open(f"{OUTPUT_DIR}/metrics.json", "w") as f:
    json.dump(metrics, f, indent=2)

rep_txt = classification_report(y_test, preds, target_names=CLASSES, digits=3, zero_division=0)
with open(f"{OUTPUT_DIR}/classification_report.txt", "w", encoding="utf-8") as f:
    f.write(rep_txt)

print("Saved:")
print(f" - {OUTPUT_DIR}/confusion_matrix_counts.png")
print(f" - {OUTPUT_DIR}/metrics.json")
print(f" - {OUTPUT_DIR}/classification_report.txt")

# ===== Main entry =====
def run(data_file: str = "/content/reddit_breakup_labeled.xlsx"):
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    best_combo, best = grid_search_and_train(data_file, out_dir=OUTPUT_DIR)
    with open(os.path.join(OUTPUT_DIR, "best_summary.json"), "w") as f:
        json.dump({"best_combo": best_combo, "best": best}, f, indent=2)
    return best_combo, best

import shutil
from google.colab import files

shutil.make_archive("distilbert_out_chunked", "zip", "distilbert_out_chunked")
files.download("distilbert_out_chunked.zip")