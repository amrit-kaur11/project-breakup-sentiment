# -*- coding: utf-8 -*-
"""Dataset Review, EDA & Baseline Models .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_KqFKFt5BTReWQxyY8UKfCB09unZAMu

# **Milestone-1: Dataset Review, EDA & Baseline Models**

**Domain-Specific Sentiment Analysis with Low-Resource NLP Techniques**

This notebook is a cleaned and 7-label Milestone‑1 deliverable: dataset review EDA, preprocessing, baseline models (VADER + LogisticRegression on TF‑IDF), export of a manual labeling sample, and saving artifacts. It is designed to run in Google Colab or local Jupyter. Set `DATA_PATH` below to your CSV file (upload to Colab or mount Drive).

**How to use:**
1. Upload `reddit_breakup_dataset_cleaned.csv` to the notebook working directory, or change `DATA_PATH` to point to your dataset in Drive.
2. Run the cells in order. The notebook creates an `outputs/` folder and saves artifacts there.

---

## *EDA*
"""

# ===== Installation =====

!pip install emoji

# ===== ENVIRONMENT SETUP =====

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sys, os, random, warnings, re, emoji

warnings.filterwarnings('ignore')

RNG = 42
from pathlib import Path
OUT_DIR = Path("outputs"); OUT_DIR.mkdir(exist_ok=True)
np.random.seed(RNG); random.seed(RNG)

# ===== IMPORTS & NLTK DOWNLOADS (guarded) =====
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.sentiment import SentimentIntensityAnalyzer

# download only if not present
def ensure_nltk(pkg):
    try:
        nltk.data.find(pkg)
    except LookupError:
        nltk.download(pkg.split('/')[-1])

ensure_nltk('corpora/stopwords')
ensure_nltk('tokenizers/punkt')
ensure_nltk('sentiment/vader_lexicon')

stop_words = set(stopwords.words('english'))
sid = SentimentIntensityAnalyzer()
nltk.download('vader_lexicon')

# ===== LOAD DATASET =====

INPUT_FILE = "reddit_breakup_dataset_cleaned.csv"
OUTPUT_FILE = "reddit_breakup_labeled.xlsx"

df = pd.read_csv(INPUT_FILE)
print("\nFirst few rows of the dataset:")
df.head(3)

# ===== SHAPE AND COLUMN NAMES =====

print("Dataset shape:", df.shape)
print("\nColumns:", df.columns)

# ===== BASIC INFO =====
df.info()

# ===== MISSING VALUES =====

print("\nMissing Values per column:\n")
df.isnull().sum()

df.describe()

# ===== TEXT LENGTH DISTRIBUTION =====

df['text_length'] = df['body'].astype(str).apply(len)
df['text_length'].describe()

plt.figure(figsize=(10,5))
sns.histplot(df['text_length'], bins=70, color='purple')
plt.xlabel("Post length(characters)")
plt.ylabel("Count")
plt.title("Distribution of Reddit Post Lengths", fontweight = "bold")
plt.show()

# ===== CORRELATION HEATMAP (numeric features only) =====
numeric_cols = ['upvotes', 'comments_count', 'author_age']
plt.figure(figsize=(8,6))
corr = df[numeric_cols].corr()
sns.heatmap(corr, annot=True, vmin=-1, vmax=1)
plt.title("Correlation Heatmap of Numeric Features", fontweight="bold")
plt.show()

# ===== WORD FREQUENCY =====
from collections import Counter

def clean_text(text):
  text = re.sub(r'[^a-zA-Z\s]', '', str(text))
  text = text.lower().split()
  text = [word for word in text if word not in stop_words]
  return text

all_words = [word for body in df['body'] for word in clean_text(body)]
word_counts = Counter(all_words)

print(word_counts.most_common(20))

#===== WORD CLOUD =====
from wordcloud import WordCloud

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(all_words))
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='lanczos')
plt.axis('off')
plt.show()

"""## *Vader Sentiment Analysis*"""

# ===== VADER SCORES =====
df['vader_score'] = df['body'].apply(lambda x: sid.polarity_scores(str(x))['compound'])

plt.figure(figsize=(10,5))
sns.histplot(df['vader_score'], bins=50, color='darkcyan')
plt.xlabel("Vader Compound Score")
plt.ylabel("Count")
plt.title("Distribution of Vader Sentiment Scores", fontweight="bold")
plt.show()

print("Sample VADER Sentiment Scores:")
print(df[['body','vader_score']].head(5))

"""## *Data Cleaning and Preprocessing*"""

# ===== QUICK SANITY & TYPE CASTING =====
required_columns = ["title", "body", "post_date", "url", "upvotes", "comments_count", "flair", "author_age", "relationship_length", "top_comments"]
for col in required_columns:
    if col not in df.columns:
        df[col] = np.nan

# Cast types
df["post_date"] = pd.to_datetime(df["post_date"], errors="coerce")
df["upvotes"] = pd.to_numeric(df["upvotes"], errors="coerce").fillna(0).astype(int)
df["comments_count"] = pd.to_numeric(df["comments_count"], errors="coerce").fillna(0).astype(int)
df["author_age"] = pd.to_numeric(df["author_age"], errors="coerce")

# ===== Step 1: ROW-LEVEL FILTERING =====

# Drop rows with null/short body
df["body"] = df["body"].astype(str).str.strip()
df = df[df["body"].str.len() >= 20]

# Deduplicate
df = df.drop_duplicates(subset=["url"], keep="first")
df = df.drop_duplicates(subset=["title", "body"], keep="first")

# ===== Step 2: TEXT NORMALIZATION =====

def normalize_text(text):
    if pd.isna(text):
        return ""
    text = str(text).strip()
    text = re.sub(r"\s+", " ", text)  # collapse spaces
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    replacements = {
        "â€™": "’", "â€˜": "‘", "â€œ": "“", "â€\x9d": "”", "â€“": "–", "â€”": "—",
        "â€¦": "…", "Â ": "", "Ã—": "×", "Ã": ""
    }
    for bad, good in replacements.items():
        text = text.replace(bad, good)
    return text

for col in ["title", "body", "top_comments"]:
    df[col] = df[col].astype(str).apply(normalize_text)

# ===== Step 3: URL, HANDLE & MARKUP NORMALIZATION =====

url_pattern = re.compile(r"https?://\S+|www\.\S+")
markup_patterns = {r"[-_*]{3,}": "—"}

def normalize_markup(text):
    text = url_pattern.sub("__URL__", text)
    for pat, rep in markup_patterns.items():
        text = re.sub(pat, rep, text)
    return text

for col in ["title", "body", "top_comments"]:
    df[col] = df[col].apply(normalize_markup)

#===== Step 4: RELATIONSHIP LENGTH EXTRACTION =====

def extract_relationship_length(text):
  # Regex to capture common time expressions (years, months, weeks, days)

    pattern = r'(\d+)\s*(years?|yrs?|months?|mos?|weeks?|wks?|days?)'
    match = re.search(pattern, str(text), re.IGNORECASE)
    if match:
        number, unit = match.groups()
        # Add 's' if the number is not 1 and 's' is not already present
        if int(number) > 1 and not unit.endswith('s'):
            return f"{number} {unit}s".lower()
        else:
            return f"{number} {unit}".lower() # return normalized lowercased match
    else:
        return "Not mentioned"

# Apply function to body column
df["relationship_length"] = df["body"].apply(extract_relationship_length)

# ===== Step 4: EMOJI & SEPCIAL CHARACTER POLICY =====

def extract_emojis(s):
    return ''.join(c for c in s if c in emoji.EMOJI_DATA)

df["emoji_list"] = df["body"].apply(lambda x: ','.join(set(extract_emojis(str(x)))))
df["emoji_count_total"] = df["body"].apply(lambda x: sum(c in emoji.EMOJI_DATA for c in str(x)))
df["emoji_count_unique"] = df["body"].apply(lambda x: len(set(extract_emojis(str(x)))))

# Convert NBSPs
df["body"] = df["body"].str.replace("\u00A0", " ", regex=False)

# ===== Step 5: CASING & PUNCTUAION NORMALIZATION =====

# text_llm: case preserved
df["text_llm"] = df["body"]

# text_tfidf: lowercased, punctuation normalized
def prepare_tfidf(text):
    text = str(text).lower()
    text = re.sub(r"!{2,}", "!!", text)
    text = re.sub(r"\?{2,}", "??", text)
    text = re.sub(r"(.)\1{2,}", r"\1", text)  # reduce elongations
    return text

df["text_tfidf"] = df["body"].apply(prepare_tfidf)

# ===== Step 6: COLUMN CREATION & FINAL TIDY =====

# Clean columns
df["clean_title"] = df["title"]
df["clean_body"] = df["body"]
df["clean_top_comments"] = df["top_comments"]

# Unified clean text
df["clean_text"] = df["clean_title"].fillna("") + "\n\n" + df["clean_body"].fillna("")

# URL metadata
df["has_url"] = df["body"].str.contains("__URL__").fillna(False)
df["url_count"] = df["body"].str.count("__URL__")

# Final NA handling
df["clean_body"] = df["clean_body"].fillna("")
df["clean_title"] = df["clean_title"].fillna("")

# Save Cleaned Dataset
df.to_excel(OUTPUT_FILE, index=False)
print(f"Cleaning completed. Saved to {OUTPUT_FILE}")

"""Creating label"""

# ===== 7-LABEL MAPPING + MANUAL SAMPLE =====

def contains_word(text, word_list):
    """Helper to match whole words instead of substrings."""
    return any(re.search(rf"\b{w}\b", text) for w in word_list)

def label_post(text):
    """
    Assigns one dominant emotion per post using priority rules.
    Priority: hopelessness > anger > sadness > relief > nostalgia > reflection > confusion > neutral
    """
    t = str(text).lower()

    # Priority classification
    if contains_word(t, ["suicide", "can't go on", "worthless", "no future", "pointless", "meaningless", "nothing matters"]):
        return "hopelessness"
    if contains_word(t, ["hate", "angry", "betray", "cheat", "liar", "selfish", "how dare", "wasted"]):
        return "anger"
    if contains_word(t, ["cry", "tears", "empty", "lost", "devastated", "lonely", "heartbroken", "empty inside"]):
        return "sadness"
    if contains_word(t, ["free", "better now", "empowered", "stronger", "happy it's over", "finally over", "relieved"]):
        return "acceptance"
    if contains_word(t, ["miss", "remember", "used to", "wish", "longing", "nostalgia", "dream about"]):
        return "nostalgia"
    if contains_word(t, ["learned", "taught me", "realized", "accept", "moving forward", "focus on myself", "growth"]):
        return "reflection"
    if contains_word(t, ["confused", "why", "don't understand", "suddenly", "out of nowhere", "unexpected"]):
        return "confusion"

    # Fallback with Vader sentiment
    score = sid.polarity_scores(t)["compound"]
    if score > 0.3:
        return "acceptance"
    elif score < -0.3:
        return "sadness"
    else:
        return "neutral"

# Apply labels
df['label'] = df['body'].astype(str).apply(label_post)

# Define the column order (important ones first)
desired_order = ["title", "body", "relationship_length", "label"]

# Reorder dataframe
df = df[desired_order + [col for col in df.columns if col not in desired_order]]

# Save the data
df.to_excel(OUTPUT_FILE, index=False)
print(f"Labeling completed. Saved to {OUTPUT_FILE}")
(df.head(2))

# ===== EXPORT STRATIFIED MANUAL LABELING SAMPLE (approx 200 rows) =====
# We'll sample up to 200 rows, stratified by label_7 if available
SAMPLE_OUT = OUT_DIR / 'manual_label_sample_label7.csv'
if 'label_7' in df.columns and df['label_7'].nunique() > 1:
    groups = df['label_7'].value_counts().to_dict()
    per_class = max(5, min(40, int(200 / max(1, len(groups)))))
    sample = df.groupby('label_7', group_keys=False).apply(lambda x: x.sample(min(len(x), per_class), random_state=RNG)).reset_index(drop=True)
else:
    # fallback simple random sample
    sample = df.sample(min(len(df), 200), random_state=RNG).reset_index(drop=True)

sample.to_csv(SAMPLE_OUT, index=False)
print('Saved manual labeling sample to', SAMPLE_OUT, 'with shape', sample.shape)

# ===== CLASS DISTRIBUTION VISUALIZATION =====

plt.figure(figsize=(10,6))
sns.countplot(data=df, x='label', order=df['label'].value_counts().index, palette='muted')
plt.title("Class Distribution of Sentiment Labels", fontweight = "bold")
plt.xlabel("Sentiment Label")
plt.ylabel("Count")
plt.xticks(rotation=30)
plt.legend()
plt.show()


# Print class counts and percentages
print("Class counts:\n", df['label'].value_counts())
print("\nClass percentages:\n", df['label'].value_counts(normalize=True) * 100)

# ===== Correlation & Numeric Feature EDA

for col in numeric_cols:
    if col in df.columns:
        plt.figure(figsize=(10,6))
        sns.boxplot(data=df, x='label', y=col, palette='Dark2')
        plt.title(f"{col.capitalize()} Distribution Across Sentiment Labels", fontweight="bold")
        plt.xlabel("Sentiment Label")
        plt.ylabel(col.capitalize())
        plt.xticks(rotation=30)
        plt.show()

# ===== BASELINE PERFORMANCE SUMMARY =====
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix

# Split the data (assuming X and y are defined from previous steps)
# If not, you would define X and y here:
X = df['clean_text']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


def get_vader_sentiment(text):
    """Assigns a sentiment label based on the VADER compound score."""
    score = sid.polarity_scores(str(text))['compound']
    if score > 0.3:
        return 'acceptance'
    elif score < -0.3:
        return 'sadness'
    else:
        return 'neutral'

# Apply VADER to the test set
y_pred_vader = X_test.apply(get_vader_sentiment)

# Calculate accuracy and F1 score of Vader
vader_accuracy = accuracy_score(y_test, y_pred_vader)
vader_f1 = f1_score(y_test, y_pred_vader, average='weighted', zero_division=0) # Added zero_division


# Logistic Regression (TF-IDF)
# Instantiate TfidfVectorizer and fit on training data
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Instantiate Logistic Regression model and train
logreg_model = LogisticRegression(max_iter=1000)
logreg_model.fit(X_train_tfidf, y_train)

# Predict on the test set
y_pred_logreg = logreg_model.predict(X_test_tfidf)

# Calculate accuracy and F1 score Logistic Regression
logreg_accuracy = accuracy_score(y_test, y_pred_logreg)
logreg_f1 = f1_score(y_test, y_pred_logreg, average='weighted', zero_division=0) # Added zero_division


# Baseline performance summary table
baseline_results = {
    "Model": ["VADER (compound polarity)", "Logistic Regression (TF-IDF)"],
    "Accuracy": [vader_accuracy, logreg_accuracy],
    "F1": [vader_f1, logreg_f1]
}
baseline_df = pd.DataFrame(baseline_results)
print("\n            === Baseline Model Comparison ===")
print(baseline_df)

# Save artifacts (models + vectorizer + canonical csv)
import joblib

joblib.dump(tfidf_vectorizer, OUT_DIR / 'tfidf_vectorizer_label7.joblib')
joblib.dump(logreg_model, OUT_DIR / 'logreg_label7.joblib')
OUT_CANON = OUT_DIR / 'reddit_breakup_labeled.xlsx'
df.to_csv(OUT_CANON, index=False)
print('Saved artifacts and canonical CSV to', OUT_DIR)

"""## Summary:

### Data Analysis Key Findings

*   The data was successfully split into training (755 samples) and testing (189 samples) sets, maintaining the class distribution.
*   VADER sentiment analysis on the test set resulted in an accuracy of 0.4074 and a weighted F1 score of 0.3038.
*   A Logistic Regression model trained on TF-IDF vectorized text achieved an accuracy of 0.3862 and a weighted F1 score of 0.2802 on the test set.
*   Comparing the two baseline models, VADER performed slightly better than Logistic Regression in terms of both accuracy (0.4074 vs 0.3862) and weighted F1 score (0.3038 vs 0.2802) on this dataset.

### Insights or Next Steps

*   Given the relatively low performance of both baseline models, exploring more advanced techniques like neural networks or experimenting with different text representations (e.g., word embeddings) could be beneficial.
*   Further analysis into the types of errors made by each model could provide insights into their limitations and guide future model selection or feature engineering efforts.

"""