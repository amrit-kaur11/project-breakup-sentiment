# -*- coding: utf-8 -*-
"""milestone3_data_centric_all_in_one.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uAE7FWN0PWxksmnUtPJBoOb-5ttp05T_

# **Milestone 3 – Data-Centric Approach** (All‑in‑One)

This notebook contains everything you need for the data‑centric milestone in one place, including:

- A helper function `augment_dataset` that reads a dataset, applies text augmentation (synonym replacement, deletion, insertion), and writes the combined data to a CSV.
- A function `collapse_labels` that maps the original fine-grained sentiment labels into four broad categories (anger, sadness, relief, confusion).
- A training function `train_tfidf_logreg` that performs a stratified train/validation/test split, optionally applies inline augmentation on the training data, vectorizes texts using TF‑IDF, and trains a Logistic Regression classifier with class‑balanced weights.  It returns the trained model and evaluation metrics.

You can customize the input/output paths and augmentation settings in the example usage cell below.
"""

!pip install nltk
!pip install transformers
!pip install sentencepiece

# Milestone 3 — Data-Centric pipeline (augment -> train TF-IDF+LR -> save artifacts)

# ===== Imports =====
import os, json, random
from typing import List, Optional

import numpy as np
import pandas as pd
import joblib

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import nltk
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger') # often needed for wordnet

# ===== Reproducibility =====
SEED = 42
random.seed(SEED)
np.random.seed(SEED)

# ===== LABEL MAPPING: Collapse original labels into Four Classes =====
LABEL_MAP = {
    "anger": "anger",
    "rage": "anger",
    "resentment": "anger",
    "sadness": "sadness",
    "grief": "sadness",
    "depression": "sadness",
    "relief": "relief",
    "acceptance": "relief",
    "confusion": "confusion",
    "mixed": "confusion",
}

def collapse_labels(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["label"] = df["label"].map(LABEL_MAP)
     # reset index after dropping unmapped rows (tidier downstream prints
    return df.dropna(subset=["label"]).reset_index(drop=True)

# -------------------- Augmentation helpers --------------------
try:
    from augment import augment_text  # optional; if missing, fallback below is used
except Exception:
    augment_text = None

def _fallback_augment_text(text: str, methods: List[str], synonym_rate: float = 0.1, noise_rate: float = 0.1) -> List[str]:
    toks = text.split()
    outs: List[str] = []
    if "deletion" in methods and len(toks) > 1:
        keep = [w for w in toks if random.random() > noise_rate] or toks[:1]
        outs.append(" ".join(keep))
    if "insertion" in methods and len(toks) > 1:
        new = []
        for w in toks:
            new.append(w)
            if random.random() < noise_rate:
                new.append(w)
        outs.append(" ".join(new))
    if "synonym" in methods:
        outs.append(text)  # no-op fallback (keeps label semantics safe without WordNet)
    return outs

def augment_once(text: str, methods: List[str], synonym_rate: float = 0.1, noise_rate: float = 0.1) -> List[str]:
    if augment_text is None:
        return _fallback_augment_text(text, methods, synonym_rate, noise_rate)
    return augment_text(text, methods, synonym_rate=synonym_rate, noise_rate=noise_rate)

# -------------------- (Optional) Back-translation --------------------
def back_translate_texts(texts: List[str], src_lang: str = "en", mid_lang: str = "de",
                         max_length: int = 256, num_beams: int = 4) -> List[str]:
    try:
        from transformers import MarianMTModel, MarianTokenizer  # requires: transformers, sentencepiece
    except Exception as e:
        raise ImportError("Install `transformers` + `sentencepiece` for back-translation.") from e

    src2mid = f"Helsinki-NLP/opus-mt-{src_lang}-{mid_lang}"
    mid2src = f"Helsinki-NLP/opus-mt-{mid_lang}-{src_lang}"

    tok_s2m = MarianTokenizer.from_pretrained(src2mid)
    mdl_s2m = MarianMTModel.from_pretrained(src2mid)
    tok_m2s = MarianTokenizer.from_pretrained(mid2src)
    mdl_m2s = MarianMTModel.from_pretrained(mid2src)

    outs: List[str] = []
    for t in texts:
        enc = tok_s2m.prepare_seq2seq_batch([t], return_tensors="pt")
        mid_ids = mdl_s2m.generate(**enc, max_length=max_length, num_beams=num_beams)[0]
        mid_text = tok_s2m.decode(mid_ids, skip_special_tokens=True)

        enc_back = tok_m2s.prepare_seq2seq_batch([mid_text], return_tensors="pt")
        back_ids = mdl_m2s.generate(**enc_back, max_length=max_length, num_beams=num_beams)[0]
        back_text = tok_m2s.decode(back_ids, skip_special_tokens=True)
        outs.append(back_text)
    return outs

# -------------------- Dataset augmentation --------------------
def augment_dataset(
    input_path: str,
    output_path: str,
    methods: Optional[List[str]] = None,
    num_augments: int = 1,
    synonym_rate: float = 0.1,
    noise_rate: float = 0.1,
    use_backtranslation: bool = False,
    backtrans_mid_lang: str = "de",
) -> pd.DataFrame:
    if methods is None:
        methods = ["synonym", "deletion"]

    if not os.path.exists(input_path):
        raise FileNotFoundError(f"Input not found: {input_path}")
    ext = os.path.splitext(input_path)[-1].lower()
    df = pd.read_excel(input_path) if ext in (".xlsx", ".xls") else pd.read_csv(input_path)

    df.columns = [c.strip() for c in df.columns]
    lowers = {c.lower(): c for c in df.columns}
    if "text" not in df.columns:
        if "text" in lowers: df = df.rename(columns={lowers["text"]: "text"})
        elif "body" in lowers: df = df.rename(columns={lowers["body"]: "text"})
        else: raise KeyError("Need a 'text' or 'body' column.")
    if "label" not in df.columns:
        if "label" in lowers: df = df.rename(columns={lowers["label"]: "label"})
        else: raise KeyError("Need a 'label' column.")

    df["text"] = df["text"].astype(str).str.strip()
    df = df.dropna(subset=["text", "label"])
    df = df[df["text"].str.len() > 0].reset_index(drop=True)

    rows: List[dict] = []
    for _, r in df.iterrows():
        t, y = str(r["text"]), r["label"]
        for _ in range(num_augments):
            for a in augment_once(t, methods, synonym_rate=synonym_rate, noise_rate=noise_rate):
                if a and a != t:
                    rows.append({"text": a, "label": y})

    if use_backtranslation:
        bt = back_translate_texts(df["text"].tolist(), mid_lang=backtrans_mid_lang)
        for y, a in zip(df["label"].tolist(), bt):
            if a:
                rows.append({"text": a, "label": y})

    aug_df = pd.DataFrame(rows)
    combined = pd.concat([df, aug_df], ignore_index=True).drop_duplicates(subset=["text"])
    combined.to_csv(output_path, index=False)
    return combined

# -------------------- Train TF-IDF + Logistic Regression --------------------
def train_tfidf_logreg(
    df: pd.DataFrame,
    val_size: float = 0.1,
    test_size: float = 0.1,
    augment: bool = False,
    num_augments: int = 1,
    methods: Optional[List[str]] = None,
    synonym_rate: float = 0.1,
    noise_rate: float = 0.1,
    save_dir: Optional[str] = None,
) -> dict:
    df["text"] = df["text"].astype(str).str.strip()
    df = df.dropna(subset=["text", "label"])
    df = df[df["text"].str.len() > 0].reset_index(drop=True)
    df = collapse_labels(df)

    texts = df["text"].tolist()
    labels = df["label"].tolist()
    x_train, x_temp, y_train, y_temp = train_test_split(
        texts, labels, test_size=val_size + test_size, stratify=labels, random_state=SEED
    )
    val_frac = val_size / (val_size + test_size) if (val_size + test_size) > 0 else 0.0
    x_val, x_test, y_val, y_test = train_test_split(
        x_temp, y_temp, test_size=1 - val_frac, stratify=y_temp, random_state=SEED
    )

    if augment and methods:
        aug_t, aug_y = [], []
        for t, y in zip(x_train, y_train):
            for a in augment_once(t, methods, synonym_rate=synonym_rate, noise_rate=noise_rate):
                aug_t.append(a); aug_y.append(y)
        x_train.extend(aug_t); y_train.extend(aug_y)

    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=20000, min_df=2)
    X_tr = vectorizer.fit_transform(x_train)
    X_va = vectorizer.transform(x_val)
    X_te = vectorizer.transform(x_test)

    clf = LogisticRegression(max_iter=1000, class_weight="balanced", n_jobs=-1)
    clf.fit(X_tr, y_train)

    def eval_split(X, y):
        yhat = clf.predict(X)
        return accuracy_score(y, yhat), f1_score(y, yhat, average="macro")

    val_acc, val_f1 = eval_split(X_va, y_val)
    test_acc, test_f1 = eval_split(X_te, y_test)

    class_names = sorted(set(labels))
    cm = confusion_matrix(y_test, clf.predict(X_te), labels=class_names)
    report = classification_report(y_test, clf.predict(X_te), output_dict=True)

    results = {
        "vectorizer": vectorizer,
        "model": clf,
        "val_accuracy": val_acc,
        "val_macro_f1": val_f1,
        "test_accuracy": test_acc,
        "test_macro_f1": test_f1,
        "confusion_matrix": cm,
        "class_names": class_names,
        "classification_report": report,
    }

    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        joblib.dump(vectorizer, os.path.join(save_dir, "tfidf_vectorizer.joblib"))
        joblib.dump(clf, os.path.join(save_dir, "logreg_model.joblib"))
        with open(os.path.join(save_dir, "metrics.json"), "w") as f:
            json.dump(
                {
                    "val": {"accuracy": val_acc, "macro_f1": val_f1},
                    "test": {"accuracy": test_acc, "macro_f1": test_f1},
                    "labels": class_names,
                    "classification_report": report,
                },
                f, indent=2
            )
        fig, ax = plt.subplots(figsize=(6, 5))
        im = ax.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
        ax.figure.colorbar(im, ax=ax)
        ax.set_xticks(np.arange(len(class_names))); ax.set_xticklabels(class_names, rotation=45, ha="right")
        ax.set_yticks(np.arange(len(class_names))); ax.set_yticklabels(class_names)
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                ax.text(j, i, cm[i, j], ha="center", va="center",
                        color="white" if cm[i, j] > cm.max()/2 else "black")
        ax.set_xlabel("Predicted"); ax.set_ylabel("True"); ax.set_title("Confusion Matrix")
        plt.tight_layout(); plt.savefig(os.path.join(save_dir, "confusion_matrix.png")); plt.show(); plt.close(fig)

    return results

# -------------------- Example usage (adjust paths as needed) --------------------
input_path = "/content/reddit_breakup_labeled.xlsx"
augmented_output_path = "/content/reddit_breakup_augmented.csv"

# 1) Create augmented CSV (optional back-translation off by default)
_ = augment_dataset(
    input_path=input_path,
    output_path=augmented_output_path,
    methods=["synonym", "deletion"],
    num_augments=1,
    use_backtranslation=False,
    backtrans_mid_lang="de",
)

# 2) Train on the (already augmented) CSV and save artifacts
ext = os.path.splitext(augmented_output_path)[-1].lower()
df_aug = pd.read_excel(augmented_output_path) if ext in (".xlsx", ".xls") else pd.read_csv(augmented_output_path)

results = train_tfidf_logreg(
    df_aug,
    val_size=0.1,
    test_size=0.1,
    augment=False,
    save_dir="/content/tfidf_output"
)

print("VAL:", results["val_accuracy"], results["val_macro_f1"])
print("TEST:", results["test_accuracy"], results["test_macro_f1"])